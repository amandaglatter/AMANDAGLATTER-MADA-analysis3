---
title: "analysis3week9"
output: html_document
---
This file uses processed_data file.

#Load libraries
```{r}
library(tidyverse)
library(dplyr)
library(broom)
library(ggplot2)
library(rsample) #for training and testing
library(tidymodels) #for modeling and recipes
```

Locate file and download data
```{r}
data_location <- here::here("data","processed_data","processed_data.rds")
dat <- readRDS(data_location)
```


"Write code that takes the data and splits it randomly into a _train and test_ that, following for instance the example in the Data Splitting section of the Get Started tidymodels tutorial."

* Training set is used to fit the model

* Testing set is used for evaluation

* Use rsample package ot create an object that tells how to split the data

* use rsample to then create dataframes for training and testing sets


```{r}
#Fic the random numbers by setting the seed, allowing the analysis to be reproducible when random numbers are used.
#We can pick any random number (?)
set.seed(48)

#Put 3/4 data into the training set
data_split <- initial_split(dat, prop =3/4)

#Create data frames for the two sets:
train_data <- training(data_split)
test_data <- testing(data_split)

```

Congratulations! We have created our training and testing data sets.

We are focusing on our _categorical outcome of interest_, which was _nausea_.We are fitting this outcome to all predictors, starting with categorical.

_Creating a Recipe._
A recipe is a description of the steps to be applied to a data set in order to prepare it for analysis.
https://recipes.tidymodels.org/reference/recipe.html
```{r}
data_recipe <- recipe(Nausea ~ ., data = train_data) #First part is the formula: Nausea is the outcome of interest and to the right of tilde are predictors (a period indicates all other variables). Second part is the data used in the model (usually training set)

```

Set a model. Set a logistical model to a categorical outcome.
```{r}
log_mod <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

```

Now we want to process the recipe using the training set, apply the recipe to the training set, and apply the recipe to the test set. To simplify the process, we use _workflows package_ from tinymodels.

```{r}
nausea_wflow <- workflow() %>%
  add_model(log_mod) %>%
  add_recipe(data_recipe)

nausea_wflow
```
In a single fucntion, we can prepare the recipe and train the model from the resulting predictors:
```{r}
nausea_fit <- nausea_wflow %>%
  fit(data=train_data)

#the following extracts the finalized recipe and fitted model objects inside.
nausea_fit %>% extract_fit_parsnip() %>%
  tidy()


```

Now we are using the trained workflow to predict with the unseen test data. For this, we use the single call "predict()".

```{r}
predict(nausea_fit, test_data)

```

The output from predict() returns Yes or No. To get back predicted class probabilities, we can specify "type = "prob"" when we use predict() or augment().
```{r}
nausea_aug_train <- augment(nausea_fit, train_data)
#This produces a tibble with predicted class probabilities.

nausea_aug_train %>% select(Nausea, .pred_Yes, .pred_No)
```

_Plotting_
Now we use the area under the ROC curve as our metric, computed using roc_curve() and roc_auc() from yardstick package.
ROC helps determine what probability cutoff should be used.Commonly, the area under the ROC curve (AUC) is used to evaluate models. If the best model immediately proceeds to the upper left corner, the area under this curve would be one while the poor model would produce an AUC in the neighborhood of 0.50. 
Then, calculate the area under the ROC curve (AUC).

```{r}
nausea_roc_train <- nausea_aug_train %>% roc_curve(truth = Nausea, .pred_Yes, event_level = "second") %>% autoplot() #**Why is event level second?
nausea_roc_train

nausea_auc_train <- nausea_aug_train %>% roc_auc(truth = Nausea, .pred_Yes, event_level = "second")
nausea_auc_train

```
The estimate of 0.797 indicates that this could be a useful model and it is about the same as the training data results.



Now test with the _test data_.
Determine predictions.
```{r}
nausea_aug_test <- augment(nausea_fit, test_data)
#This produces a tibble with predicted class probabilities.

nausea_aug_test %>% select(Nausea, .pred_Yes, .pred_No)
```

_Plotting_
Plot the test data using the model.
Then, calculate the area under the ROC curve (AUC).

```{r}
nausea_roc_test <- nausea_aug_test %>% roc_curve(truth = Nausea, .pred_Yes, event_level = "second") %>% autoplot() #**Why is event level second?
nausea_roc_test

nausea_auc_test <- nausea_aug_test %>% roc_auc(truth = Nausea, .pred_Yes, event_level = "second")
nausea_auc_test

```
The area under the curve is about 0.686, so the model might be useful. _Should this number be closer to that of the training data??_


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


Letâ€™s re-do the fitting but now with a model that only fits the main predictor to the categorical outcome.

```{r}
data_recipe_nose <- recipe(Nausea ~ RunnyNose, data = train_data) 
```

Set a model again. Set a logistical model to a categorical outcome.

```{r}
log_mod_nose <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

```

Now we want to process the recipe using the training set, apply the recipe to the training set, and apply the recipe to the test set. To simplify the process, we use _workflows package_ from tinymodels.

```{r}
nausea_nose_wflow <- workflow() %>%
  add_model(log_mod_nose) %>%
  add_recipe(data_recipe_nose)

nausea_nose_wflow
```

In a single function, we can prepare the recipe and train the model from the resulting predictors:
```{r}
nausea_nose_fit <- nausea_nose_wflow %>%
  fit(data=train_data)

#the following extracts the finalized recipe and fitted model objects inside.
nausea_nose_fit %>% extract_fit_parsnip() %>%
  tidy()


```

Now we are using the trained workflow to predict with the unseen test data. For this, we use the single call "predict()".

```{r}
predict(nausea_nose_fit, test_data)

```
The output from predict() returns Yes or No. To get back predicted class probabilities, we can specify "type = "prob"" when we use predict() or augment().
```{r}
nausea_nose_aug_train <- augment(nausea_nose_fit, train_data, type = "prob")

nausea_nose_aug_train
#This produces a tibble with predicted class probabilities.
```
Now we use the area under the ROC curve as our metric, computed using roc_curve() and roc_auc() from yardstick package.
ROC helps determine what probability cutoff should be used.Commonly, the area under the ROC curve (AUC) is used to evaluate models. If the best model immediately proceeds to the upper left corner, the area under this curve would be one while the poor model would produce an AUC in the neighborhood of 0.50. 

```{r}
nausea_nose_roc_train <- nausea_nose_aug_train %>% roc_curve(truth = Nausea, .pred_Yes, event_level = "second") %>% autoplot()

nausea_nose_roc_train

```
This is almost perfectly 0.5. This means the model is no good.

Calculating the area under the curve:
```{r}

nausea_nose_auc_train <- nausea_nose_aug_train %>% roc_auc(truth = Nausea, .pred_Yes, event_level = "second")

nausea_nose_auc_train

```
The value of 0.501 reinforces this: the model is no good.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~`
Now let's use the _test data_!

First we predict the probabilities
```{r}
predict(nausea_nose_fit, test_data, type = "prob")

```

Now we use augment to turn this into a data frame.
```{r}

nausea_nose_aug_test <- augment(nausea_nose_fit, test_data)
nausea_nose_aug_test %>% select(Nausea, .pred_Yes, .pred_No)

```

Plot as ROC, then calculate the AUC (area under curve)
```{r}
nausea_nose_roc_test <- nausea_nose_aug_test %>% roc_curve(truth = Nausea, .pred_Yes, event_level = "second") %>% autoplot()

nausea_nose_roc_test

nausea_nose_auc_test <- nausea_nose_aug_test %>% roc_auc(truth = Nausea, .pred_Yes, event_level = "second")

nausea_nose_auc_test
```

The area under the curve is 0.48, which is pretty close to the trained value. Still, this is not a good model.




#Nicholas' Code Starts Here

```{r}
# Creates a simple recipe that fits our continuous outcome of interest to all predictors 
bodytmp_rec <- 
  recipe(BodyTemp ~ ., data = train_data)

# Set a model as we did in the previous exercise
lr_mod <- 
  linear_reg() %>% 
  set_engine("lm")


# Use the workflow() package to create a
# simple workflow that fits a linear model
# to all predictors using the glm function
bodytmp_wflow <- 
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(bodytmp_rec)

# Fitting the model
bodytmp_fit <- 
  bodytmp_wflow %>% 
  fit(data = train_data)

# Extracting Model/Recipes with Parsnip
bodytmp_fit %>% 
  extract_fit_parsnip() %>% 
  tidy()
```



```{r}

# Obtaining Predictions
predict(bodytmp_fit, train_data)

bodytmp_aug <- 
  augment(bodytmp_fit, train_data)

bodytmp_aug %>%
  select(BodyTemp)


```

##Calculating RMSE for Complex Model on Training Data
```{r}
# Calculating Root RMSE 
rmse_train <- bodytmp_aug %>% 
  rmse(truth = BodyTemp, .pred)

rmse_train
```


```{r}

# Now on Test Data


# Obtaining Predictions
predict(bodytmp_fit, test_data)

bodytmp_aug <- 
  augment(bodytmp_fit, test_data)

bodytmp_aug %>%
  select(BodyTemp)

```



##Calculating RMSE for Complex Model on Test Data
```{r}
# Calculating Root RMSE 
rmse_test <- bodytmp_aug %>% 
  rmse(truth = BodyTemp, .pred)

rmse_test 

```


###NOW WITH ONLY ONE PREDICTOR

```{r}

# Creates a simple recipe that fits our categorical outcome of interest to all predictors 
bodytmp_runny_rec <- 
  recipe(BodyTemp ~ RunnyNose, data = train_data) 


# Set a model as we did in the previous exercise
lr_mod <- 
  linear_reg() %>% 
  set_engine("lm")


# Use the workflow() package to create a
# simple workflow that fits a linear model
# to all predictors using the glm function
bodytmp_runny_wflow <- 
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(bodytmp_runny_rec)

# Fitting the model
bodytmp_runny_fit <- 
  bodytmp_runny_wflow %>% 
  fit(data = train_data)

# Extracting Model/Recipes with Parsnip
bodytmp_runny_fit %>% 
  extract_fit_parsnip() %>% 
  tidy()

```


```{r}
# For Training Data

# Obtaining Predictions
predict(bodytmp_runny_fit, train_data)

bodytmp_runny_aug <- 
  augment(bodytmp_runny_fit, train_data)

bodytmp_runny_aug %>%
  select(BodyTemp)

```


##Calculating RMSE for Simple Model from Training Data
```{r}
# Calculating Root RMSE 
rmse_runny_train <- bodytmp_runny_aug %>% 
  rmse(truth = BodyTemp, .pred)

rmse_runny_train
```

```{r}

# For Test Data

# Obtaining Predictions
predict(bodytmp_runny_fit, test_data)

bodytmp_runny_aug <- 
  augment(bodytmp_runny_fit, test_data)

bodytmp_runny_aug %>%
  select(BodyTemp)
```


##Calculating RMSE for Simple Model from Test Data
```{r}

# Calculating Root RMSE 
rmse_runny_test <- bodytmp_runny_aug %>% 
  rmse(truth = BodyTemp, .pred)

rmse_runny_test

```


###Conclusion
The RMSE for the complex model on the training data was 1.14 and the RMSE for the complex model on the test data was 1.06. The RMSE for the simple model on the training data was 1.24 and the RMSE for the simple model on the test data was 1.03.  Based on the RMSE values, I would conlcude that the complex model does a better job becuase the RMSE is closer in value between the training and test models and overall lower in the complex model compared with the simple model. 
